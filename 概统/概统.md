[toc]

# 第1章 概率论的基本概念

确定性现象 统计规律性 随机现象

## 1.1 随机试验

**随机试验的性质：**
1.可以在相同的条件下重复地进行
2.每次实验可能的结果不止一个，并且能事先明确实验的所有可能结果
3.进行一次实验之前不能确定哪一个结果会出现

## 1.2 样本空间、随机事件

我们将随机试验E地所有可能结果组成的集合称为E的**样本空间**，E的每个结果，称为**样本点**。

我们称实验E的样本空间S的子集为E的**随机事件**，简称**事件**。由一个样本点组成的单点集，称为**基本事件**。S称为**必然事件**，空集$ \phi$称为**不可能事件**。

**事件间关系：**
1.$A \subset B$，则称事件B包含事件A，这指的是事件A发生必导致事件B发生。若$ A = B$，则称事件A与事件B相等。
2.事件$ A \cup B$称为事件A与事件B的**和事件**，当且仅当A，B其中一个发生，事件$ A \cup B$发生。
3.事件$ A \cap B$称为事件A与事件B的**积事件**，当且仅当A，B都发生，事件$ A \cap B$发生。
4.事件$ A - B$称为事件A与事件B的**差事件**，当且仅当A发生，B不发生时$ A - B$发生。
5.若$ A \cap B = \phi$，则称事件A与B是**互不相容的**，或**互质的**，这指的是事件A与事件B不能同时发生，基本事件是两两互不相容的。
6.若$ A \cup B = S$且$ A \cap B = \phi$，则称事件A与事件B互为**逆事件**，又称事件A与事件B互为**对立事件**，这指的是对每次实验而言，事件A、B中必有一个事件发生，且仅有一个发生。事件A的对立事件为$ \overline{A}$。$ \overline{A} = S - A$。

交换律：$ A \cup B = B \cup A \quad A \cap B = B \cap A$
结合律：$ A \cup ( B \cup C ) = ( A \cup B ) \cup C \quad A \cap ( B \cap C ) = ( A \cap B ) \cap C$
分配律：$ A \cup ( B \cap C ) = ( A \cup B ) \cap ( A \cup C) \quad A \cap ( B \cup C ) = ( A \cap B ) \cup ( A \cap C)$
德摩根律：$ \overline{ A \cup B } = \overline{A} \cap \overline{B} \quad \overline{ A \cap B } = \overline{A} \cup \overline{B}$

## 1.3 频率与概率

在相同的条件下，进行了n次实验，在这n次实验中，事件A发生的次数$n_A$称为事件A发生的**频数**，比值$ n_A/n$称为事件A发生的**频率**，并记为$ \int_n(A)$。

设E是随机实验，S是他的样板空间，对于E的每一事件A赋予一个实数，几维P(A)，称为事件A的概率。
**性质：**
1.$P( \phi)=0$
2.有限可加性：$P( A_1 \cup A_2 \cup A_3 \cdots A_n ) = P(A_1) + P(A_2) +  P(A_3) + \cdots + P(A_n)$
3.$ P( B - A ) = P( B - AB ) = P(B) - P(AB) \quad$若$ A \subset B$，则$ P( B - A ) = P( B - AB ) = P(B) - P(A)$
4.对任意两事件A、B，有$ P( A \cup B ) = P(A) + P(B) - P(AB)$
5.$ P( A_1 \cup A_2 \cup \cdots \cup A_n) = \sum_{ i = 1}^n P(A_i) - \sum _{ 1 \leqslant i < j \leqslant n } P( A_i A_j) + \sum _{ 1 \leqslant i < j < k \leqslant n } P( A_i A_j A_k ) + \cdots + (-1)^{n-1} P( A_1 A_2 \cdots A_n)$

## 1.4 等可能概型（古典概型）

**性质：**
1.实验样本空间只包含有限个元素。
2.实验中每个基本事件发生呢的可能性相同。

放回抽样和不放回抽样概率相同。

超几何分布：$ p = \frac{ C _D ^k C _{N - D} ^{n - k}}{ C _N ^n}$

## 1.5 条件概率

在A发生的情况下B发生的**条件概率**：$ P(B|A) = \frac{P(AB)}{P(A)}$
$ P( \bigcup _{ i = 1 } ^{ \infty } B_i | A ) = \sum _{ i = 1 } ^{\infty}P( B_i | A )$($ B_i$互不相容时，若非，则用容斥排除)

乘法定理：$ p(A) > 0$，则$ P(AB) = P( B | A ) P(A) \quad P(ABC) = P( C | AB ) P( B | A ) P(A)$

**全概率公式:**
设实验E的样本空间为S，A为E的事件，$ B_1, B_2, \cdots B_n$为S的一个划分，且$ P(B_i) > 0$则：
$ P(A) = P(A B_1) + P(A B_2) + \cdots + P(A B_n) = P(A|B_1) P(B_1) + P(A|B_2) P(B_2) + \cdots + P(A|B_n) P(B_n)$为**全概率公式**。(此时P(A)不易求得，但P($B_i$)和P($A|B_i$)易得)

**贝叶斯公式：**
$ P( B_i | A ) = \frac{ P( B_i A )}{P(A)} = \frac {P( A | B_i ) P(B_i)}{ \sum _{ j = 1} ^{n} P( A | B_j) P(B_j)}$

## 1.6 独立性

定义：设A、B是两事件，如满足等式$P(AB) = P(A) P(B)$，则称事件A、B**相互独立**，简称A、B独立。
若A、B相互独立，则$ P( B | A ) = \frac{ P(AB) }{ P(A) } = P(B)$。
一般，设$ A_1,A_2,\cdots,A_n$是n个事件，如果对于其中任意2个，任意3个，$\cdots$，任意n个时间的积事件，都等于各事件概率之积，则称事件$ A_1,A_2,\cdots,A_n$**相互独立**。

# 第2章 随机变量及其分布

## 2.1 随机变量

这节啥也没有

## 2.2 离散型随机变量及其分布

有些随机变量，它全部可能的取值是有限个或可列无限多个，这种随机变量称为**离散型随机变量**。分布律可用表格写也可以直接写。

（一）（0-1）分布：**X~b(1,p)**

分布律为：$ P \{ X = k \} = p^k (1-p)^{ 1-k },k=0,1$，称X服从以p为参数的（0-1）分布或两点分布。

（二） 伯努利实验、二项分布：**X~b(n,p)**

设实验E只有两个可能结果：$A及 \overline{A}$，则称E为**伯努利实验**，将E重复地进行n次，则称这一串重复的独立实验为**n重伯努利实验**。
分布律：$ P \{ x = k \} = C_{n} ^k p^k q^{n-k},k = 0,1,2,\cdots,n$，称X服从参数n，p的**二项分布**。

（三） 泊松分布：**X~ $\pi(\lambda)$**

分布律：$ P \{ X = K \} = \frac{ \lambda ^ k e ^{-\lambda}}{k!}, k=0,1,2,\cdots$，称X服从参数为$\lambda$的**泊松分布**。

**泊松定理**(用泊松分布逼近二项分布)：设$ \lambda > 0$是一个常数，n为任意正整数，设$ n p_n = \lambda$，则对任一固定非负整数k，有：
$ \lim \limits _{ n \to \infty} C _{n} ^k p_n^k (1-p_n)^{n-k} = \frac{ \lambda ^ k e ^{-\lambda}}{k!}$(由泰勒展开可得)此时n很大，$p_n$很小。
上式说明以n，p为参数的二项分布的概率值可以由参数为$ \lambda = n p$的泊松分布的概率值近似，上式也可用来做二项分布的近似计算。一般当$ n \geq 20,p \leqslant 0.05$时用 $ \frac{ \lambda ^ k e ^{-\lambda}}{k!}$替代$  C_{n} ^k p^k q^{n-k}$效果颇佳。

## 2.3 随机变量的分布函数

定义：设X是一个随机变量,x是任意实数，函数$ F(x) = P \{ X \leqslant X \} , -\infty < x < \infty$，称为X的**分布函数**。
$ p\{ x_1 < X \leqslant x_2 \} = P \{ X \leqslant x_2 \} - P\{ X \leqslant x_1\} = F(x_2) - F(x_1)$。
F(x)是个不减函数。$ F( -\infty ) = 0 , F(\infty) = 1$。
F(x)恰是非负函数f(t)在[0,$ \infty$]上的积分，这种情况下我们称X为连续性随机变量。

## 2.4连续性随机变量及其概率密度

若对于随机变量X的分布函数F(x)，存在非负可积函数f(x)，使对于任意实数x有:
$ F(x) = \int _{-\infty}^x f(t)dt$。
**性质：**
1.$ f(x) \geq 0$
2.$ \int _{ -\infty } ^{\infty} f(x)dx = 1$
3.$ p\{ x_1 < X \leqslant x_2 \} = P \{ X \leqslant x_2 \} - P\{ X \leqslant x_1\} = F(x_2) - F(x_1) = \int _{x_1}^{x_2} f(x)dx$
4.若f(x)在点x处连续，则有F'(x) = f(x)。
由定义可知，改变概率密度f(x)在个别点的函数值不影响分布函数F(x)的取值，因此，并不在乎改变概率密度在个别点上的值。

（一）均匀分布：**X~U(a,b)**

$ f(x)=\begin{cases}  \frac{1}{b-a} ,a < x < b \\ 0,others\end{cases}$，称X在区间[a,b]上满足**均匀分布**，记作X~U(a,b).

$ F(x)=\begin{cases}  0,x<a \\  \frac{x-a}{b-a} ,a \leqslant x < b \\ 1,x \geq b \end{cases}$

（二）指数分布：**X~E($\lambda$)($\lambda = \frac{1}{\theta}$)**

若连续型随机变量X的概率密度为
$ f(x) = \begin{cases} \frac{1}{\theta} e^{-\frac{x}{\theta}} ,x > 0\\ 0 ,others \end{cases}$,其中$ \theta > 0$为常数，则称X服从参数为$ \theta$的**指数分布**。

$ F(x)=\begin{cases}  1-e^{ - \frac{-x}{\theta}} ,x > 0 \\ 0,others\end{cases}$

指数分布具有**无记忆性**:$ P(X>s+t|X>s) = P(X>t)$

（三）正态分布

若连续性随机变量X的概率密度为:
$ f(x) = \frac{1}{\sqrt{ 2 \pi} \sigma } e^{ - \frac{ (x-\mu)^2}{ 2 \sigma ^2}} , - \infty < x < \infty$，其中$ \mu,\sigma ( \sigma > 0)$为常数， 则称X服从参数为$ \mu,\sigma$的**正太分布**或**高斯分布**，记为X~N($ \mu, \sigma ^2$)。

$ F(x) = \frac{1}{\sqrt{2 \pi} \sigma} \int^{x}_{ - \infty} e^{- \frac{(t- \mu)^2}{2 \sigma ^2}}dt$

1.概率密度曲线关于$ x= \mu$对称，这表明对于任意h>0有$ P\{ \mu - h < X \leqslant \mu \} = P( \mu < X \leqslant \mu + h )$
2.当$ x = \mu$时取到最大值 $ f(\mu) = \frac{1}{\sqrt{2 \pi} \sigma}$

特别的当$ \mu = 0$,$ \sigma = 1$时称随机变量X服从**标准正态分布**，其概率密度为$ \varphi(x) = \frac{1}{ \sqrt {2 \pi}} e^{ -\frac{x^2}{2}} \quad \phi(x) = \frac{1}{\sqrt{2 \pi}} \int _{-\infty} ^x e^{ - \frac{t^2}{2}}dt \quad \phi(0) = \frac{1}{2}$

若$X \sim N(\mu,\sigma^2)$，则$ Z = \frac{X - \mu}{\sigma} \sim N(0,1)$。、

设$ X \sim N(\mu,\sigma^2),P \{ X < t\} = \phi(\frac{t-\mu}{\sigma})$

$P \{ \mu - \sigma < X < \mu + \sigma \} = \phi(1) - \phi(-1) = 2 \phi(1) - 1 = 68.26 \%$
$P \{ \mu - 2 \sigma < X < \mu + 2 \sigma \} = \phi(2) - \phi(-2) = 2 \phi(2) - 1 = 95.44 \%$
$P \{ \mu - 3 \sigma < X < \mu + 3 \sigma \} = \phi(3) - \phi(-3) = 2 \phi(3) - 1 = 99.74 \%$
**$3 \sigma$法则**：可知尽管正太变量的取值范围是$(-\infty,\infty)$，但它的值落在$( \mu - 3 \sigma , \mu + 3 \sigma)$内几乎是肯定的。

设X~N(0,1)，若$ Z_a$满足条件$ P\{ X > Z_a \} = a , 0 < a < 1$，则称点$Z_a$为标准正态分布的**上a分布点**。

| a     | 0.001 | 0.005 | 0.01  | 0.025 | 0.05  | 0.1   |
|-------|-------|-------|-------|-------|-------|-------|
| $Z_a$ | 3.090 | 2.576 | 2.326 | 1.960 | 1.645 | 1.282 |

## 2.5 随机变量的函数的分布

设随机变量X具有概率密度$f_x(x),-\infty < x <\infty$，又设函数g(x)处处可导且恒有g’(x)>0(或g’(x)<0)，则Y=g(x)是连续型随机变量，其概率密度为
$ f_y(y) = \begin{cases} f_x [h(y)]|h'(y)| ,\alpha < y < \beta \\ 0 , others \end{cases}$，即用h(y)换掉x，在乘上h'(y)。

其中$ \alpha = min \{ g(- \infty) , g(\infty) \} , \beta = max \{ g(- \infty) , g(\infty) \}$,h(x)是g(x)的反函数。

证：
Y = g(X) 在$ (\alpha,\beta)$内取值，当y$ \leqslant \alpha$时，$ F_Y(y) = P(Y \leqslant y) = 0$，当y$ \geq \beta$时，$ F_Y(y) = P(Y \leqslant y) = 1$.
当$ \alpha < y < \beta$时，$ F_Y(y) = P\{ Y \leqslant y\} = P \{ g(X) \leqslant y \} = P\{ X \leqslant h(y) \} = F_X [h(y)]$
当g'(x)<0时，仅需将h'(x)改为-h'(x)即可。
若在有限区间[a,b]，则$ \alpha = min \{ g(a) , g(b) \} , \beta = max \{ g(a) , g(b) \}$.

# 第3章 多维随机变量及其分布

## 3.1 二维随机变量

设(X,Y)是二位随机变量，对于任意实数x，y，二元函数F(x,y) = P{X$ \leqslant x$,Y $ \leqslant y$}，称为二维随机变量(X,Y)的**分布函数**，或称为随机变量X和Y的**联合分布函数**。

性质：
1.F(x,y)是变量x和y的不减函数，及对于任意固定的y，当$x_2 > x_1$时，F($x_2$,y) $ \geq$ F($x_1$,y)，换了也一样。
2.$ 0 \leqslant F(x,y) \leqslant 1$,且
$对于任意固定的y，F(-\infty,y) = 0.$
$对于任意固定的x，F(x,-\infty) = 0.$
$F(-\infty,-\infty) = 0,F(\infty,\infty) = 1$
$ + \infty 仅F(\infty,\infty) = 1 这一性质$
3.$ F(x,y)关于x，y都右连续。$
4.对于任意$ (x_1,y_1),(x_2,y_2),x_1<x_1,y_1<y_2$，下述不等式成立：
$ F(x_2,y_2) - F(x_2,y_1) - F(x_1,y_2) + F(x_1,y_1) \geq 0$，即二维前缀和。

如果二维随机变量(X,Y)全部可能取到的值是有限对或可列无限多对，则(X,Y)是**离散型的随机变量**。
我们称$P\{ X = x_i , Y = y_i \} = p_{ij}，i,j = 1,2,\cdots$为二维离散型随机变量(X,Y)的**分布律**，或称为随机变量X和Y的**联合分布律**。

对于二维随机变量(X,Y)的分布函数F(x,y)，如果存在非负可积函数f(x,y)使对任意x，y有$ F(x,y) = \int _{ - \infty}^y \int _{ - \infty} ^ x f(u,v) du dv$，则称(X,Y)是**连续型的二维随机变量**，函数f(x,y)称为二维随机变量(X,Y)的**概率密度**，或称为随机变量X和Y的**联合概率密度**。

性质：
1.$f(x,y) \geq 0$.
2.$\int _{ - \infty}^{\infty} \int _{ - \infty}^{\infty} f(x,y) dx dy = F(\infty,\infty) = 1$
3.设G是XOY平面上的区域，点(X,Y)落在G内的概率为：$ P \{ (X,Y) \in G \} = \int \int _G f(x,y) dx dy$
4.若f(x,y)在点(x,y)连续，则有$ \frac{ \partial ^2 F(x,y)}{\partial x \partial y} = f(x,y)$

## 3.2 边缘分布

$ F_x(x),F_y(y)$称为二维随机变量(X,Y)关于X和关于Y的**边缘分布函数**。
$ F_x(x) = P\{X \leqslant x\} = P \{ X \leqslant x , Y \leqslant \infty \} = F(x,\infty)$，也就是说，只要在F(x,y)中令$ y \to \infty$就能得到$ F_x(x)$，x同理。

离散型随机变量：
$ F_x(x) = F(x,\infty) = \sum _{x_i \leqslant x} \sum _{j = 1} ^{\infty} p_{ij}$，y同理。
X的分布律为：$ P \{ X = x_i\} = \sum _{j=1}^{\infty}p_{ij} , i = 1,2,\cdots$
Y的分布律为：$ P \{ Y = y_i\} = \sum _{i=1}^{\infty}p_{ij} , j = 1,2,\cdots$
记$ p_{i \cdot} = \sum _{j=1}^{\infty}p_{ij} = P \{ X = x_i\} , i = 1,2,\cdots$
$ p_{\cdot j} = \sum _{i=1}^{\infty}p_{ij} = P \{ Y = y_i\} , j = 1,2,\cdots$
分别称$p_{i \cdot }, i = 1,2,\cdots$和$ p_{ \cdot j}, j = 1,2,\cdots$为(X,Y)关于X和关于Y的**边缘分布律**。

连续型随机变量：
$ F_x(x) = F(x,\infty) = \int _{-\infty} ^x [\int _{- \infty } ^{\infty} f(x,y) dy] dx$
X的概率密度：$ f_x(x) = \int _{-\infty} ^{\infty} f(x,y) dy$
Y的概率密度：$ f_y(y) = \int _{-\infty} ^{\infty} f(x,y) dx$
分别称$ f_x(x) ,f_y(y)$为(X,Y)关于X和Y的**边缘概率密度**。

**二维正态分布**的**概率密度**为：
$ f(x,y) = \frac{1}{ 2 \pi \sigma _1 \sigma _2 \sqrt {1 - \rho ^2}} exp \{ \frac{ - 1}{2 ( 1 - \rho ^2)} [ \frac{(x - \mu _1)^2 }{ \sigma _1 ^2} - 2 \rho \frac{(x-\mu _1)(x-\mu_2)}{\sigma_1 \sigma _2} + \frac{(y - \mu _2)^2}{ \sigma _2 ^2}] \}$，参数为$ \mu _1 ,\mu_2 ,\sigma_1 ,\sigma_2 ,\rho$，记为$ (X,Y) \sim N(\mu_1,\mu_2,\sigma_1,\sigma_2,\rho)。$

**边缘概率密度**为：
$ f_x(x) = \frac{1}{ \sqrt{2 \pi} \sigma_1} e ^{ - \frac{(x - \mu_1)^2}{2 \sigma_1 ^2} } , -\infty < x < \infty$
$ f_y(y) = \frac{1}{ \sqrt{2 \pi} \sigma_2} e ^{ - \frac{(x - \mu_2)^2}{2 \sigma_2 ^2} }, - \infty < y < \infty$

可知二维正态分布的两个边缘分布都是一维正态分布，且都不依赖参数$\rho$，即对于给定的$\mu_1,\mu_2,\sigma_1,\sigma_2$，不同的$\rho$对应不同的二维正态分布，他们的边缘分布是一样的，并表明单由关于X和关于Y的边缘分布，一般来说是不能确定随机变量X和Y的联合分布的。

## 3.3 条件分布

设(X,Y)是二维离散型随机变量，对于固定的j，若$ P\{ Y =y_i \} \geq 0$，则称$ P\{X = x_i |Y = y_j \} = \frac{ P\{ X =x_i, Y = y_j \}}{ P\{Y = y_j \}} = \frac{P_{ij}}{P_{\cdot j}}$为在$ Y = y_j$条件下随机变量X的**条件分布律**，Y同理。

设二维随机变量(X,Y)的概率密度为f(x,y),(X,Y)关于Y的边缘概率密度为$ f_y(y)$。若对于固定的y，$f_y(y) > 0$则称$ f_{x|y}(x|y) = \frac{f(x,y)}{f_y(y)}$为在Y = y条件下X的**条件概率密度**，称$ \int _{ - \infty} ^ x f _{x|y} ( x|y ) dx = \int _{ - \infty } ^x \frac{f(x,y)}{f_y(y)} dx$为在 Y = y条件下X的条件分布函数，记为$ P \{ X \leqslant x  Y \leqslant y\} 或 F_{x|y}(x|y)$，其余类似。

## 3.4 相互独立的随机变量

设F(x,y)及$F_x(x),F_y(y)$分别是二维随机变量(X,Y)的分布函数及边缘分布函数，若对所有x，y有:$ P\{ X \leqslant x , Y \leqslant y\} = P\{ X \leqslant x \} P\{ Y \leqslant y\}$，即$ F(x,y) = F_x(x) F_y(y)$，则称随机变量X和Y是**相互独立的**。
(X,Y)为连续型随机变量时，X和Y独立的条件等价于$ f(x,y) = f_x(x) f_y(y)$，在平面上除面积为0的集合外处处成立。
(X,Y)为离散型随机变量时，X和Y独立的条件等价于$ P \{ X = x_i, Y = y_j\} = P\{ X = x_i\} P\{ Y = y_j \}$.

对于二维正态随机变量(X,Y)，X和Y相互独立的充要条件是参数$\rho = 0$。

n维随机变量有的定理：
设$(X_1,X_2,\cdots,X_m)$和$(Y_1,Y_2,\cdots,Y_n)$相互独立，则$X_i(i = 1,2,\cdots,m)$和$Y_j(j = 1,2,\cdots,n)$相互独立，又若h,g是连续函数，则$ h(X_1,X_2,\cdots,X_m)$和$ g(Y_1,Y_2,\cdots,Y_n)$相互独立。

## 3.5 两个随机变量的函数的分布

（一）Z = X + Y的分布

概率密度为 ：$ f_{X+Y} (Z) = \int _{-\infty} ^{\infty} f(z-y,y) dy = \int _{-\infty} ^{\infty} f(x,z-x) dx$
若X和Y相互独立：$ f_{X+Y}(z) = \int _{-\infty}^{\infty} f_X(z-y) f_Y(y) dy = \int _{-\infty}^{\infty} f_X(x) f_Y(z-x) dx = f_X * f_Y$，此式记为**卷积公式**。(要注意其中x和z-x的范围，以分段函数写出所有情况)。
$ F_Z(z) = \int _{-\infty}^{\infty}[\int _{-\infty}^{z} f(u-y,y)du]dy = \int _{-\infty} ^{z} [\int _{-\infty}^{\infty} f(u-y,y)dy]du$

**正态分布：**
有限个相互独立的正态随机变量的线性组合仍服从正态分布：若$ X_i \sim N(\mu_i,\sigma_i^2)(i = 1,2,\cdots,n)$它们的和$ Z = X_1 + X_2 + \cdots + X_n$仍服从正态分布，$ Z \sim N( \mu_1 + \mu_2 + \cdots + \mu_n , \sigma_1^2 + \sigma_2^2 + \cdots + \sigma_n^2 )$。

**$\tau$分布：**
$X \sim \tau(\alpha , \theta)$概率密度：
$ f_X(x) = \begin{cases} \frac{1}{\theta ^\alpha \tau(\alpha)} x^{\alpha - 1}e^{ \frac{-x}{\theta}} , x > 0\\ 0 ,others \\ \end{cases}$
$ X \sim \tau(\alpha,\theta) , Y \sim \tau(\beta ,\theta)，X+Y \sim \tau(\alpha + \beta , \theta).$

（二）Z = $ \frac{Y}{X}的分布、Z = XY的分布$

概率密度：
$ f_{ \frac{Y}{X}}(z) = \int _{-\infty}^{\infty} | x | f(x,xz) dx$
$ f_{XY}(z) = \int _{-\infty}^{\infty} | \frac{1}{x} | f(x,\frac{z}{x}) dx$
若X和Y独立：
$ f_{ \frac{Y}{X}}(z) = \int _{-\infty}^{\infty} | x | f_x(x) f_y(xz) dx$
$ f_{XY}(z) = \int _{-\infty}^{\infty} | \frac{1}{x} | f_x(x) f_y(\frac{z}{x}) dx$

（三）M = max{X,Y}及 N = min{X,Y}的分布

M = max{X,Y}:
$ P \{ M \leqslant z \} = P \{ X \leqslant z , Y \leqslant z\}$
若X和Y独立：
$ F_{max}(z) = P \{ M \leqslant z \} = P \{ X \leqslant z , Y \leqslant z\} = P \{X \leqslant z\} P \{Y \leqslant z\} = F_x(z) F_y(z)$
不独立：$ F_{max}(z) = F(z,z)$.

推导步骤解释：
分布即$ M \leqslant z$的概率，因为M对应max{X,Y}，所以即是$ X \leqslant z 且 Y \leqslant Z$的概率。

N = min{X,Y}:
$ F_{min}(z) = P \{ N \leqslant z \} = 1 - P \{ N > z \} = 1 - P \{ X > z , Y > z \} = 1 - P \{ X > z \} P \{ Y > z \} = 1 - [ 1 - F_x(z)] [ 1 - F_y(z)]$

推导步骤解释：
分布即$ N \leqslant z$的概率，因为N对应min{X,Y}，因此$ N \leqslant z$不宜表示，转化为$ 1 - P \{ N > z \}$的概率，即原来的相反区间，最小值小于等于z的相反为都大于z，都大于即为$ X > z 且 Y > z$的概率，再用$ 1 - P \{ X \leqslant z \} 和 1 - P \{ Y \leqslant z \}$换回来。

n个相互独立的随机变量：
$M = max \{X_1,X_2,\cdots,X_n \}:$
$ F_{max}(z) = F_{x_1}(z) F_{x_2}(z) \cdots F_{x_n}(z)$
$N = min \{X_1,X_2,\cdots,X_n \}:$
$ F_{min}(z) = 1 - [ 1 - F_{x_1}] [ 1 - F_{x_2}] \cdots [ 1 - F_{x_n}]$

# 第4章 随机变量的数字特征

数学期望、方差、相关系数、矩

## 4.1 数学期望

离散型随机变量X的分布律为$ P\{ X = x_k\} = p_k , k = 1,2,\cdots，E(x) = \sum _{k = 1} ^{\infty} x_k p_k(\sum _{k = 1} ^{\infty} x_k p_k 绝对收敛)$为随机变量X的**数学期望**。

连续型随机变量X的概率密度为$f(x)，E(x) = \int _{- \infty} ^{\infty} x f(x) dx(\int _{- \infty} ^{\infty} x f(x) dx绝对收敛)$为随机变量X的**数学期望**。

数学期望，简称**期望**，又称为**均值**。

定理：
设Y是随机变量X的函数：Y = g(x)。
1.X是离散型随机变量，$ E(Y) = E(g(x)) = \sum _{k = 1} ^{\infty} g(x_k) p_k$。
2.X是连续型随机变量，$ E(Y) = E(g(x)) = \int _{-\infty} ^{\infty} g(x) f(x) dx$。
此定理的重要意义在于我们求E(Y)时，不必算出Y的分布律或概率密度，而只需要利用x的分布律或概率密度就可以了，就是直接用g(x)替换掉初始的x就行了。
上述定理在二维的时候同样适用，都是直接换即可。

随机变量(X,Y)的概率密度为$f(x,y)$:
$ E(X) = \int _{-\infty}^{\infty} \int _{-\infty}^{\infty} x f(x,y) dy dx = \int _{-\infty} ^{\infty} x [ \int _{-\infty} ^{\infty}f(x,y) dy] dx = \int _{-\infty} ^{\infty} x f_y(x,y) dx$
$ E(Y) = \int _{-\infty}^{\infty} \int _{-\infty}^{\infty} y f(x,y) dx dy$
$ E(XY) = \int _{-\infty}^{\infty} \int _{-\infty}^{\infty} x y f(x,y) dx dy$
$ E(\frac{1}{XY}) = \int _{-\infty}^{\infty} \int _{-\infty}^{\infty} \frac{1}{xy} f(x,y) dx dy$

公式解释：
第一列可以直接看为用定理替代，E(x)可以看为X的边缘分布律的数学期望。

计算的重要性质：
1.C为常数，E(C) = C。
2.E(CX) = CE(X)。
3.E(X+Y) = E(X) + E(Y)。
4.E(XY) = E(X) E(Y)。

## 4.2 方差

设X是一个随机变量，若$E \{ [X - E(X)]^2\}$存在，则称$E \{ [X - E(X)]^2\}$为X的**方差**，记为D(X)或Var(X)，即$ D(X) = Var(X) = E \{ [X - E(X)]^2\}$。
$ \sigma (x) = \sqrt{D(X)}$称为**标准差**或**均方差**。

离散型随机变量：$ D(X) = \sum _{ k = 1 } ^{\infty} [x_k - E(X)]^2 p_k$
连续型随机变量：$ D(x) = \int _{-\infty} ^{\infty} [ x - E(X)]^2 f(x) dx$
由性质得：$ D(X) = E \{ [X - E(X)]^2\} = E \{ X ^2 - 2 X E(X) + [E(X)]^2 \} = E(X^2) - 2 E(X)E(X) + [E(X)]^2 = E(X^2) - [E(X)]^2$

计算的重要性质：
1.$D(C) = 0$。
2.$D(CX) = C^2D(X)$。
3.$D(C+X) = D(X)$。
4.$D(X+Y) = D(X) + D(Y) + 2 E\{ [X - E(X)] [Y - E(Y)]\} = D(X) + D(Y) （相互独立）$
5.D(X) = 0 的充要条件是X以概率1取常数E(X)，即P{X = E(X)} = 1.

**总结：**

| 分布类名 | 表示形式                                         | 数学期望                       | 方差                               |
|----------|--------------------------------------------------|--------------------------------|------------------------------------|
| 泊松分布 | $ X \sim \pi (\lambda)$                          | $\lambda$                      | $\lambda$                          |
| 二项分布 | $ X \sim b(n,p)$                                 | np                             | np(p-1)                            |
| 均匀分布 | $ X \sim U(a,b)$                                 | $ \frac{a+b}{2}$               | $ \frac{( b - a )^2}{12} $         |
| 指数分布 | $ X \sim E(\lambda)(\lambda = \frac{1}{\theta})$ | $ \frac{1}{\lambda} 或 \theta$ | $\frac{1}{\lambda^2} 或 \theta ^2$ |
| 正态分布 | $ X \sim N(\mu,\sigma^2)$                        | $\mu$                          | $\sigma^2$                         |

**切比雪夫不等式:**
设随机变量X具有数学期望$E(X) = \mu$，方差$ D(X) = \sigma ^2$，则对任意正数$\varepsilon$，不等式：$ P \{ | X - \mu | \geq \varepsilon \} \leqslant \frac{\sigma ^2}{ \varepsilon ^2}$成立。

## 4.3 协方差及相关系数

$ E\{[X - E(X)][Y - E(Y)]\}$称为随机变量X和Y的**协方差**，记为$Cov(X,Y)$，即：$Cov(X,Y) = E\{[X - E(X)][Y - E(Y)]$，而$ \rho _{xy} = \frac{Cov(X,Y)}{\sqrt{D(X) \sqrt{D(Y)}}}$称为随机变量的**相关系数**。

$ D(X+Y) = D(X) + D(Y) + 2 Cov(X,Y)$
$ Cov(X,Y) = E(XY) - E(X)E(Y)$

协方差性质：
1.$Cov(aX,bY) = abCov(X,Y)$。
2.$Cov(X_1 + X_2,Y) = Cov(X_1,Y) + Cov(X_2,Y)$
3.$Cov(aX + C_1,bY + C_2) = abCov(X,Y)$。

考虑以X的线性函数a+bX来近似表示Y，我们以均方误差$e = E[(Y - ( a_0 + b_0 X)^2] = (1-\rho_{xy}^2) D(Y)$来衡量以a+bX近似表示Y的好坏程度，e越小表示a+bX与Y的近似程度越好。

当|$ \rho_{xy}$|较大时e较小，表明就线性关系来说较密切，当|$ \rho_{xy} = 1$|时，表示存在线性关系。$ \rho_{xy} = 0$时，称X和Y不相关。(注意正负号选择)。、

X和Y相互独立$\to \rho _{xy} = 0 \to$X和Y不相关；但不相关却不一定独立。当(X,Y)服从二维正态分布时，X和Y不相关与X和Y相互独立是等价的，即第5个参数$\rho = 0$。

## 4.3 矩、协方差矩阵

设X和Y是随机变量，若$E(X^k)，k = 1,2,\cdots$存在，称它为X的**k阶原点矩**，简称**k阶矩**；若$E\{[X - E(X)]^k \}，k = 2,3,\cdots$存在,称它为X的**k阶中心矩**；若$E(X^k Y^l),k,l = 1,2,\cdots$存在，称它为X和Y的**k+l阶混合矩**；若$ E\{[X - E(X)]^k [Y - E(Y)]^l \},k,l = 1,2,\cdots$存在，称它为X和Y的**k+l阶混合中心矩**。
显然：X的数学期望E(X)是X的一阶原点矩，方差D(X)是X的二阶中心矩，协方差Cov(X,Y)是X和Y的二阶混合中心矩。

设n维随机变量$(X_1,X_2,\cdots,X_n)$的二阶混合中心矩$ c_{ij} = Cov(X_i,X_j) = E\{[X_i - E(X_i)] [X_j - E(x_j)]\},i,j = 1,2,\cdots,n$都存在，则称矩阵：
C =$\left\{ \begin{matrix} C_{11} & C_{12} & \cdots & C_{1n}\\ C_{21} & C_{22} & \cdots & C_{2n} \\ \vdots & \vdots & \cdots & \vdots \\ C_{n1} & C_{n2} & \cdots & C_{nn} \end{matrix} \right\}$为n维随机变量的**协方差矩阵**。由于$C_{ij} = C_{ji}$，所以为对称矩阵。

n维正态随机变量$(X_1,X_2,\cdots,X_n)$的概率密度为：
$ f(x_1,x_2,\cdots,x_n) = \frac{1}{(2\pi)^{\frac{n}{2}} (detC)^{\frac{1}{2}}} exp \{ -\frac{1}{2} (X - \mu) ^T C ^{-1} (X - \mu) \}$
其中：$ X = \left \{ \begin{matrix} x_1 \\ x_2 \\  \vdots \\ x_n \end{matrix} \right \}$,$ \mu = \left \{ \begin{matrix} \mu_1 \\ \mu_2 \\  \vdots \\ \mu_n \end{matrix} \right \} = \left \{ \begin{matrix} E(X_1) \\ E(X_2) \\  \vdots \\ E(X_n) \end{matrix} \right \}$

n维正态随机变量具有以下四条重要性质：
1.n维正态随机变量$(X_1,X_2,\cdots,X_n)$的每一个分量$X_i,i = 1,2,\cdots,n$都是正态随机变量；反之，若$X_1,X_2,\cdots,X_n$都是正态随机变量，且相互独立，则$(X_1,X_2,\cdots,X_n)$是n维正态随机变量。
2.n维正态随机变量$(X_1,X_2,\cdots,X_n)$服从n维正态分布的充要条件是$X_1,X_2,\cdots,X_n$的任意线性组合$ l_1X_1 + l_2X_2 + \cdots + l_nX_n$服从一维正态分布。
3.若$(X_1,X_2,\cdots,X_n)$服从n维正态分布，设$ Y_1,Y_2,\cdots,Y_n$是$X_j(j = 1,2,\cdots,n)$的线性函数，则$(Y_1,Y_2,\cdots,Y_n)$也服从多维正态分布。这一性质称为正太变量的线性变换不变性。
4.设$(X_1,X_2,\cdots,X_n)$服从n维正态分布，则“$X_1,X_2,\cdots,X_n$相互独立”和“$X_1,X_2,\cdots,X_n$两两不相关等价”。

# 第5章 大数定理及中心极限定理

## 5.1 大数定理

**弱大数定理(辛钦大数定理)：**
设$X_1,X_2,\cdots,X_n$是相互独立，服从同一分布的随机变量序列，且具有数学期望$E(X_k) = \mu (k = 1,2,\cdots )$，作前n个变量的算术平均$\frac{1}{n} \sum_{ k = 1}^{n}X_k$，对任意$\varepsilon > 0$，有$ \lim \limits _{n \to \infty} P \{ | \frac{1}{n} \sum _{k = 1 } ^{n} X_k - \mu | < \varepsilon \} = 1$

证：
$E(\frac{1}{n} \sum _{ k = 1 } ^{n} X_k ) = \frac{1}{n} \sum _{ k = 1 } ^{n} E(X_k) = \mu$。
$D(\frac{1}{n} \sum _{ k = 1 } ^{n} X_k) = \frac{1}{n^2} \sum_{ k = 1 } ^{n} D(X_k) = \frac{\sigma ^2}{n}$。
由切比雪夫不等式得：
$ 1 \geq P\{ | \frac{1}{n} \sum _{ k = 1 }^{n} X_k -\mu | < \varepsilon \} \geq 1 - \frac{ \frac{\sigma ^2}{n}}{\varepsilon^2}$。由$ n \to \infty$可知，$ \lim \limits _{n \to \infty} P \{ | \frac{1}{n} \sum _{k = 1 } ^{n} X_k - \mu | < \varepsilon \} = 1$。

此式表明，对任意正数$\varepsilon$，当n充分大时，不等式$ |\frac{1}{n} \sum _{k = 1} ^{n} X_k - \mu | < \varepsilon$成立的概率很大。通俗的说，辛钦大数定理是说，对于独立同分布且具有均值$\mu$的随机变量$ X_1,X_2,\cdots,X_n$，当n很大时他们的算术平均$\frac{1}{n} \sum _{k = 1} ^{n} X_k$很可能接近于$\mu$。

设$ Y_1,Y_2,\cdots,Y_n$是一个随机变量序列，a是一个常数，若对于任意正数$\varepsilon$，有$ \lim \limits _{n \to \infty} P \{ | Y_n - a | < \varepsilon \} = 1$，则称序列$ Y_1,Y_2,\cdots,Y_n$**依概率收敛于a**，记为$ Y_n \underrightarrow{P} a$.

依概率收敛的序列有以下性质：
设$X_n \underrightarrow{P} a , Y_n \underrightarrow{P} b$，又设函数g(x,y)在点(a,b)连续，则$g(X_n,Y_n) \underrightarrow{P} g(a,b)$.

因此，辛钦大数定理又可叙述为：设变量$X_1,X_2,\cdots,X_n$相互独立，服从同一分布且具有数学期望$E(X_k) = \mu ,k = 1,2,\cdots$，则序列$ \overline{X} = \frac{1}{n} \sum _{k = 1} ^{n} X_k$依概率收敛于$\mu$，即$ \overline{X} \underrightarrow{P} \mu$.

推论：
**伯努利大数定理：**
设$f_A$是n次独立重复实验中事件A发生的次数，p是事件A在每次实验中发生的概率，则对任意正数$\varepsilon$，有$\lim \limits _{ n \to \infty} P \{ | \frac{f_A}{n} - p | < \varepsilon \} = 1 或 \lim \limits _{ n \to \infty} P \{ | \frac{f_A}{n} - p | \geq \varepsilon \} = 0$

表示在n充分大时，事件“频率$\frac{f_A}{n}$与概率p的偏差小于$\varepsilon$”实际上几乎是必定要发生的。这就是我们所说的概率稳定性的真正含义。在实际应用中，当试验次数很大时，便可以用事件的频率代替事件的概率。

## 5.2 中心极限定理

**定理一（独立同分布的中心极限定理）：**
设随机变量$X_1,X_2,\cdots,X_n$相互独立，服从同一分布，且具有数学期望和方差：$E(X_k) = \mu,D(X_k) = \sigma^2 > 0 , k = 1,2,\cdots$，则随机变量和$ \sum _{k = 1} ^n X_k$的标准化变量$ Y_n = \frac{\sum_{ k = 1}^n X_k - E(\sum_{k = 1}^n X_k)}{ \sqrt{D( \sum _{k = 1}^n X_k)}} = \frac{\sum_{k = 1}^n X_k - n \mu}{ \sqrt{n} \sigma}$在n充分大时，有$ \frac{ \sum _{k = 1} ^ n X_k - n \mu }{ \sigma \sqrt{n}} \sim N(0,1)$，可转化为$ \frac{ \frac{1}{n} \sum _{k = 1} ^ n X_k - \mu }{ \frac{\sigma } {\sqrt{n}} }\sim N(0,1)$，即$ \frac{ \overline{X} - \mu }{ \frac{\sigma } {\sqrt{n}} } \sim N(0,1)$ 或 $ \overline{X} \sim N(\mu , \frac{\sigma^2}{n})$。

也就是说，均值为$\mu$，方差为$\sigma^2$地独立同分布的随机变量$X_1,X_2,\cdots,X_n$的算术平均值$ \overline{X}$在n充分大时近似地服从均值为$\mu$，方差为$\frac{\sigma^2}{n}$的正态分布，这一结果是数理统计中大样本统计推断的基础。

此式中:$Y_n$的分布函数$F_n(x)$对任意x满足$ \lim \limits _{n \to \infty}  F_{n}(x) = \lim \limits _{n \to \infty} P \{ \frac{ \sum _{k = 1}^n X_k - n \mu}{\sqrt{n} \sigma } \leqslant x \} = \Phi(x)$

**定理二（李雅普诺夫定理）：**
设随机变量$X_1,X_2,\cdots,X_n$相互独立，他们具有数学期望和方差$E(X_k) = \mu _k , D(X_k) = \sigma _k^2 > 0 , k = 1,2,\cdots$，记$ B_n ^ 2 = \sum _{ k = 1} ^n \sigma_k ^2$，则随机变量之和$ \sum _{k = 1} ^ n X_k$的标准化变量$ Z_n = \frac{\sum _{k = 1}^n - E(\sum _{k = 1}^n X_k)}{ \sqrt{D(\sum _{k = 1}^n X_k)}} = \frac{ \sum _{k = 1}^n X_k - \sum _{k = 1}^n \mu _k}{B_n}$,可知在n很大时，近似地服从正态分布N(0,1)，由此，当n很大时，$ \sum _{k = 1}^n X_k = B_n Z_n + \sum_{k = 1}^n \mu_k$近似的服从正态分布$ N(\sum _{k = 1}^n \mu_k , B_n ^2)$。也就是说，无论各个随机变量$X_k , k = 1,2,\cdots$服从什么分布，只要满足定理的条件，那么他们的和$\sum _{k = 1}^n$当n很大时，就近似地服从正态分布。

此式中:$Y_n$的分布函数$F_n(x)$对任意x满足$ \lim \limits _{n \to \infty}  F_{n}(x) = \lim \limits _{n \to \infty} P \{ \frac{ \sum _{k = 1}^n X_k - \sum_{k = 1} ^n \mu_k}{B_n ^2} \leqslant x \} = \Phi(x)$。

**定理三（棣莫弗-拉普拉斯定理）：**
此定理为定理一的特殊情况：
设随机变量$ \eta _{n} ,n = 1,2,\cdots$服从参数n，p( 0 < p < 1)的二项分布。由于二项分布$ \eta _{n}$可以拆分成n个相互独立，服从同一(0-1)分布的诸随机变量$ X_1,X_2,\cdots,X_n$之和，即有$ \eta_n = \sum _{k = 1}^n X_k,E(X_k) = p,D(X_k) = p(1-p)$,所以有定理一可得：$ \eta_{n}$的分布函数$F_n(x)$对任意x满足$ \lim \limits _{n \to \infty} F_{n}(x) = \lim \limits _{n \to \infty} P \{ \frac{ \eta _{n} - np}{\sqrt{np(1-p)} } \leqslant x \} = \Phi(x)$。

这个定理表明，正态分布是二项分布的极限分布，当n充分大时，我们可以利用此式来计算二项分布的概率。

# 第6章 样本及抽样分布

## 6.1 随机样本

我们将实验全部可能的观察值称为**总体**，每一个可能的观察值称为**个体**，总体所包含的个体的个数称为总体得到**容量**，容量有限的称为**有限总体**，无限的称为**无限总体**，有些有限总体的容量很大，我们可以认为他是一个无限总体。

设X是具有分布函数F的随机变量，若$X_1,X_2,\cdots,X_n$是具有同一分布函数F的，相互独立的随机变量，则称$X_1,X_2,\cdots,X_n$为从分布函数F得到的**容量为n的简单随机样本**，简称**样本**。它们的观察值$x_1,x_2,\cdots,x_n$称为**样本值**，又称为X的n个**独立的观察值**。

## 6.2 直方图和箱线图

~~老师跳了我也跳~~

## 6.3 抽样分布

定义：设$X_1,X_2,\cdots,X_n$是来自总体X的一个样本，$ g(X_1,X_2,\cdots,X_n)$是$ X_1,X_2,\cdots,X_n$的函数，若g中不含任何参数，则称$ g(X_1,X_2,\cdots,X_n)$是一**统计量**。

因为$ X_1,X_2,\cdots,X_n$是随机变量，而统计量$ g(X_1,X_2,\cdots,X_n)$是随机变量的函数，因此统计量是一个随机变量。设$ x_1,x_2,\cdots,x_n$是相应于$X_1,X_2,\cdots,X_n$的样本值，则称$ g(x_1,x_2,\cdots,x_n)$是$ g(X_1,X_2,\cdots,X_n)$的观察值。

**样本平均值：** $ \overline{X} = \frac{1}{n} \sum _{i = 1} ^n X_i$
**样本方差：** $ S^2 = \frac{1}{n - 1} \sum_{ i = 1} ^n ( X_i - \overline{X}) ^2 = \frac{1}{n - 1} ( \sum_{ i = 1} ^n X_i ^2 - n \overline{X} ^2)$
**样本标准差：** S
**样本k阶(原点)矩：** $ A_k = \frac{1}{n} \sum _{i = 1} ^n X_i ^k , k = 1,2,\cdots$
**样本k阶中心矩：** $ B_k = \frac{1}{n} \sum _{i = 1}^n (X_i - \overline{X}) ^k , k = 2,3,\cdots$

若总体X的k阶矩$E(X^k) = \mu$ 存在，则当$ n \to \infty$时，$ A_k \underrightarrow{P} \mu_k , k = 1,2,\cdots$，进而$ g(A_1,A_2,\cdots,A_n) \underrightarrow{P} g(\mu_1,\mu_2,\cdots,\mu_n)$，其中g为连续函数。这就是下一张所介绍的矩估计法的理论依据。

**经验分布函数：**
设$ X_1,X_2,\cdots,X_n$是总体F的一个样本，用$ S(x) , - \infty < x < \infty$表示$ X_1,X_2,\cdots,X_n$中不大于x的随机变量的个数。定义经验分布函数$ F_n(x)$为$ F_n(x) = \frac{1}{n} S(x) , - \infty < x < \infty$。当$ n \to infty$时，经验分布函数的任一个观察值$ F_n(x)$与总体分布函数$F(X)$只有微小的差别，从而实际上可以当作$F(x)$来使用。

统计量的分布称为**抽样分布**，在使用统计量进行统计推断时常需知道它的分布，在总体分布已知时，抽样分布是确定的，然而要求出统计量的精确分布，一般来说是困难的。以下介绍来自正太总体的几个常用统计量的分布：

1.$ \chi ^2$**分布:**
设$ X_1,X_2,\cdots,X_n$是来自总体N(0,1)的样本，则统计量$ \chi ^2 = X_1^2 + X_2 ^2 + \cdots + X_n ^2$服从自由度为n的$ \chi ^2$**分布**，记作$ \chi ^2 \sim \chi^2(n)$。(自由度指的是下式右端包含的独立变量的个数)
$ \chi ^2 (n)$分布的概率密度：
$ f(y) = \begin{cases}  \frac{1}{2^{ \frac{n}{2}} \tau( \frac{n}{2})} y^{ \frac{n}{2} - 1} e ^{ \frac{-n}{2}} , y > 0 \\ 0 ,others \\ \end{cases}$
由2.5的例三可知$ X_i ^2 \sim \chi ^2(1) \sim \tau( \frac{1}{2} , 2) , i = 1,2,\cdots$，由$ \tau$分布的可加性可知，$ \chi ^2 = \sum _{ i = 1} ^n \sim \tau{\frac{n}{2}}$，因此可得上述概率密度。

**$ \chi^2$分布的可加性**：$\chi _1 ^2 \sim \chi (n_1) , \chi _2 ^2 \sim \chi (n_2) , 且 \chi _1 ^2 , \chi _2 ^2$相互独立，则有$ \chi _1 ^2 + \chi _2 ^2 \sim \chi ^2 (n_1 + n_2)$
**$ \chi ^2$分布的数学期望和方差**：$ E( \chi ^2 ) = n , D( \chi ^ 2 ) = 2n$

$ \chi ^2$**分布的上分位点**：n<40时查表，大于时$ \chi _{a} ^2 \approx \frac{1}{2} ( z_a + \sqrt{2n+1}) ^2,z_a$是标准正态分布的上a分位点。

2.$ t$**分布**
设$X \sim N(0,1) , Y \sim \chi^2(n)$且X，Y相互独立，则称随机变量$ t = \frac{X}{ \sqrt{ \frac{Y}{n}}}$服从自由度为n的$ t$**分布**，记为$ t \sim t(n)$。

t分布又称为**学生氏分布**,t(n)的分布概率密度为$ h(t) = \frac{\Tau[\frac{(n+1)}{2}]}{ \sqrt{ \pi n } \Tau( \frac{n}{2})} ( 1 + \frac{t^2}{n})^{ - \frac{(n+1)}{2}} , -\infty < t < \infty$

**t分布的上a分位点：** n<=45时查表，大于时$ t_a(n) \approx z_a$。
$ t_{1-a} = t_{a}$

3.**$F$分布**
设$ U \sim \chi^2 ( n_1 ) , V \sim \chi^2 ( n_2 )$，且U,V相互独立，则称随机变量$ F = \frac{ \frac{U}{n_1}}{ \frac{V}{n_2}}$为服从自由度为$ ( n_1,n_2)$的**F分布** ，记为$ F \sim F( n_1 , n_2)$。

概率密度太麻烦不想写。

$ F( n_1 , n_2) = \frac{1}{F( n_2 , n_1 )}$

**F分布的上分位点：** 可查表。$ F_{1-a} (n_1,n_2) = \frac{1}{F_a(n_2,n_1)}$

4.正态总体的样本均值和样本方差的分布

$ E( \overline{X}) = \mu , D( \overline{X}) = \frac{ \sigma^2}{n} , E(s^2) = \sigma^2 , D(s^2) = \frac{2(n-1)^3}{\sigma^4} (证明：D(\frac{(n-1) s^2}{\sigma^2}) = D( \chi^2 (n-1)) = 2(n-1))$.

**定理一：** 设$ X_1 , X_2 , \cdots , X_n$是来自正态总体$ N( \mu , \sigma^2)$的样本，$ \overline{X}$是样本均值，则有$ \overline{X} \sim N(\mu,\frac{\mu}{\sigma^2})$.

**定理二：** 设$ X_1 , X_2 , \cdots , X_n$是来自正态总体$ N( \mu , \sigma^2)$的样本，$ \overline{X} , S^2$分别是样本均值和样本方差，则有$ \frac{( n - 1 ) S^2}{\sigma^2} \sim \chi^2(n-1)$和$ \overline{X} 与 S^2$相互独立。

**定理三：** 设$ X_1 , X_2 , \cdots , X_n$是来自正态总体$ N( \mu , \sigma^2)$的样本，$ \overline{X} , S^2$分别是样本均值和样本方差，则有$\frac{ \overline{X} - \mu}{\frac{S}{ \sqrt{n}}} \sim t(n-1)$。

证：由定理一二可知$ \frac{ \overline{X} - \mu}{\frac{\sigma}{ \sqrt{n}}} \sim N(0,1) , \frac{( n - 1 ) S^2}{\sigma^2} \sim \chi^2(n-1)$，所以$ \frac{ \frac{ \overline{X} - \mu}{\frac{\sigma}{ \sqrt{n}}}}{ \sqrt{ \frac{\frac{( n - 1 ) S^2}{\sigma^2}}{n - 1}}} = t(n-1)$

**定理四：** 设$ X_1 , X_2 , \cdots , X_n$和$ Y_1 , Y_2 , \cdots , Y_n$分别是来自正态总体$ N( \mu_1 , \sigma_1^2)$和$ N( \mu_2 , \sigma _2 ^2 )$的样本，且两个样本相互独立，设$ \overline{X} = \frac{1}{n_1} \sum_{i = 1} ^{n_1} X_i, \overline{Y} = \frac{1}{n_2} \sum_{i = 1} ^{n_2} Y_i$分别是这两个样本的均值；$ S_1^2 = \frac{1}{n_1 - 1} \sum_{i = 1}^{n_1} ( X_i - \overline{X}) , S_2^2 = \frac{1}{n_2 - 1} \sum_{i = 1}^{n_2} ( Y_i - \overline{Y}) ^2$分别是这两个样本的样本方差。则有:
1.$ \frac{S_1^2 / S_2^2}{  \sigma_1^2 / \sigma_2^2} \sim F(n_1 - 1 , n_2 - 1)$，证明由定理二可得。
2.当$ \sigma_1 = \sigma_2 = \sigma$时，$ \frac{(\overline{X} - \overline{Y}) - (\mu_1 - \mu_2)}{ S_w \sqrt{ \frac{1}{n_1} + \frac{1}{n_2}}} \sim t(n_1 + n_2 - 2)$，其中
$ S_w^2 = \frac{(n_1 - 1) S_1^2 + (n_2 - 1) S_2^2}{n_1 + n_2 - 2}$。

# 第7章 参数估计

## 7.1 点估计

点估计问题的一般提法：设总体X的分布函数$F(x;\theta)$的形式已知，$\theta$是待估计参数，$X_1,X_2,\cdots,X_n$是X的一个样本，$x_1,x_2,\cdots，x_n$是相应的一个样本值。点估计问题就是要构造一个适当的统计量$ \hat{\theta} (X_1,X_2,\cdots,X_n)$，用它的观察值$ \hat{\theta} (x_1,x_2,\cdots,x_n)$作为未知函数$ \theta$的近似值吗，我们称$ \hat{\theta} (X_1,X_2,\cdots,X_n)$为$\theta$的**估计量**，称$ \hat{\theta} (x_1,x_2,\cdots,x_n)$为$ \theta$的**估计值**，，在不混淆的情况下统称为**估计**。

（一）**矩估计法**

定义：已知$A_l = \frac{1}{n} \sum_{i = 1}^n = X_i^l$依概率收敛于相应的总体矩$ \mu_l$，样本矩的连续函数依概率收敛于相应的总体矩的连续函数。我们用样本矩作为相应的总体量的估计量，用样本矩的连续函数作为相应的总体矩的连续函数的估计量，此方法称为**矩估计法**。

设X为连续性随机变量，其概率密度为$f(x;\theta_1,\theta_2,\cdots,\theta_n)$，或X为离散型随机变量，其概率密度为$P \{ X = x \} = p(x;\theta_1,\theta_2,\cdots,\theta_n)$，那么总体X的前k阶矩为:
$ \mu_l = E(X^l) = \int_{-\infty} ^{\infty} x^l f(x;\theta_1,\theta_2,\cdots,\theta_n) dx$（连续型）
 或$ \mu_l = E(X^l) = \sum _{x \in R_X} x^l p(x;\theta_1,\theta_2,\cdots,\theta_n)$（离散型）

具体做法：
$ \begin{cases} \mu_1 = \mu_1 ( \theta_1 , \theta_2 , \cdots , \theta_n) \\ \mu_2 = \mu_2 ( \theta_1 , \theta_2 , \cdots , \theta_n) \\ \quad \vdots \\ \mu_n = \mu_n ( \theta_1 , \theta_2 , \cdots , \theta_n) \end{cases}$
可转化为：
$ \begin{cases} \theta_1 = \theta_1 ( \mu_1 , \mu_2 , \cdots , \mu_n) \\ \theta_2 = \theta_2 ( \mu_1 , \mu_2 , \cdots , \mu_n) \\ \quad \vdots \\ \theta_n = \theta_n ( \mu_1 , \mu_2 , \cdots , \mu_n) \end{cases}$
分别用$ A_i$替换上式的$ \mu_i$就以$ \hat{\theta_i} = \theta_i (A_i , A_2 , \cdots , A_n)$作为$ \theta_i$的估计量，称为**矩估计量**，他的观察值称为**矩估计值**。

注意$ \frac{1}{n} \sum_{i = 1}^n X_i^2- \overline{X}^2 = \frac{1}{n} \sum_{i = 1}^n ( X_I - \overline{X}) ^2$，可替代。$E(X^2) = D(x) + [E(X)]^2$。

均匀分布$[a,b]：\hat{a} = \overline{X} - \sqrt{\frac{3}{n} \sum_{i = 1} ^n (X_i - \overline{X})^2} , \hat{b} = \overline{X} + \sqrt{\frac{3}{n} \sum_{i = 1} ^n (X_i - \overline{X})^2}$。

总体X的均值$\mu$的估计量：$ \hat{\mu} = \overline{X}$。
总体X的方差$ \sigma^2$的估计量：$ \hat{ \sigma^2} =  \frac{1}{n} \sum_{i = 1}^n (X_i - \overline{X})^2$。

（二）**最大似然估计法**

**似然函数：**$ L(\theta) = L(x_1,x_2,\cdots,x_n ; \theta) = \prod_{i = 1}^n p(x_i ; \theta) (离散型) = \prod_{i = 1}^n f(x_i ; \theta) (连续型)$

定义：固定样本观察值$ x_1,x_2,\cdots,x_n$，在$\theta$的可能取值范围$\Theta$内挑选使似然函数$ L(x_1,x_2,\cdots,x_n ; \theta)$达到最大的参数值$ \hat{\theta}$，作为参数$ \theta$的估计值，即取$ \hat{\theta}$使$ L(x_1,x_2,\cdots,x_n ; \hat{\theta}) = max L(x_1,x_2,\cdots,x_n ; \theta) , \theta \in \Theta$，这样的得到的$ \hat{\theta}$与样本值$ x_1,x_2,\cdots,x_n$有关，常记为$ \hat{\theta} ( x_1,x_2,\cdots,x_n)$，称为参数$ \theta$的**最大似然估计值**，而对应的统计量$ \hat{\theta}(X_1,X_2,\cdots,X_n)$称为参数$ \theta$的**最大似然估计量**。

由此我们可知，最大似然估计法就是求某一$ \theta$使$ L(\theta)$取得最大值，即$ \frac{d L(\theta)}{d \theta} = 0$又由于$L(\theta)$一般为幂函数，不便求导。所以，引入**对数似然方程**：$ \frac{d lnL(\theta)}{d \theta}$求解，解值相同。当然，若原式可以直接看出最大值，则不需求导。

最大似然估计法也适用于分布中含多个未知参数$ \theta_1 , \theta_2 , \cdots , \theta_n$的情况，这时，分别令$ \frac{  \partial L}{ \partial \theta_i} = 0 或 \frac{ \partial ln L}{ \partial \theta _i} = 0$解上述由k个方程组成的方程组，即可得到各未知数$ \theta_i$的最大似然估计值$ \hat{\theta_i}$，方程组称为称为**对数似然方程组**。

均匀分布[a,b]的最大似然估计量：$ \hat{a} = min X_i , 1 \leqslant i \leqslant n ; \hat{b} = max X_i , 1 \leqslant i \leqslant n$。

最大似然估计的**不变性**：设$ \theta$的函数$ u = u (\theta), \theta \in \Theta$具有单值反函数$ \theta = \theta (u) , u \in U$，又假设$ \hat{\theta}$是X的概率分布中参数$ \theta$的最大似然估计，则$ \hat{u}$是$ u(\theta)$的最大似然估计。

## 7.2 *基于截尾样本的最大似然估计

~~打星跳~~

## 7.3 估计量的评选标准

**无偏性:** 若估计量$ \hat{\theta} = \hat{\theta} (X_1,X_2,\cdots,X_n)$的数学期望$ E(\theta)$存在，且对任意$ \theta \in \Theta$有$ E( \hat{\theta}) = \theta$，则称$ \hat{\theta}$是$ \theta$的**无偏估计量**。

估计量的无偏估计值是说对于某些样本值，由这一估计量的得到的估计值相对于其真值来说偏大，有些偏小，反复将这一估计量使用多次，就平均来说误差为0。在科学技术中$E(\theta) - \theta$称作以$ \theta$作为$ \theta$的估计的系统误差。无偏差估计的实际意义就是无系统误差。

无论总体服从什么分布，样本均值$ \overline{X}$是总体均值$ \mu$的无偏估计；样本方差$ S^2 = \frac{1}{n-1} \sum_{i = 1}^n E(X_i^k) , i = 1,2,\cdots,n$是总体方差的无偏估计。

要证明某一值A是B的无偏估计量，即证明E(A) = B。

**有效性:** 现在比较参数$ \theta$的两个无偏估计量$ \hat{\theta_1}和 \hat{\theta_2}$，如果在样本容量n相同的情况下，$ \hat{\theta_1}$的观察值较$ \hat{\theta_2}$更密集在真值$ \theta$附近，即$ D( \hat{\theta_1} ) \leqslant D( \hat{\theta_2})$，我们称为$ \hat{\theta_1}$较$ \hat{\theta_2}$**有效**。

**相合性:** 设$ \hat{\theta} (X_1,X_2,\cdots,X_n)$为参数$ \theta$的估计量，若对于任意$ \theta \in \Theta$，当$ n \to \infty$时$ \hat{\theta} (X_1,X_2,\cdots,X_n)$依概率收敛于$ \theta$，则称$ \hat{\theta}$为$ \theta$的**相合估计量**。
即对于任意$ \theta \in \Theta$都满足：对任意$ \varepsilon > 0$，有$ \lim \limits _{n \to \infty} P \{ | \hat{\theta} - \theta| < \varepsilon \} = 1$。

相合性谁对一个估计量的基本要求，若估计量不具有相合性，那么不论将样本容量n取的多大，都不能将$ \theta$估计得足够准确，这样的估计量是不可取的。

## 7.4 区间估计

**置信区间：** 设总体X的分布函数$ F(x;\theta)$含有一个未知参数$ \theta$，$ \theta \in \Theta$，对于给定值$a(0<a<1)$，若由来自X的样本$ X_1 , X_2 , \cdots ,X_n$确定的两个统计量$ \underline{\theta} = \underline{\theta} ( X_1,X_2,\cdots,X_n)$和$ \overline{\theta} = \overline{\theta} ( X_1,X_2,\cdots,X_n) , (\underline{\theta} < \overline{\theta})$，对任意$ \theta \in \Theta$满足$ P \{ \underline{\theta} ( X_1,X_2,\cdots,X_n) < \theta < \overline{\theta} ( X_1,X_2,\cdots,X_n) \} \geqslant 1 - a$，则称随机区间$ ( \underline{\theta} , \overline{\theta})$是$ \theta$的置信水平为1-a的**置信区间**，$ \underline{\theta}$和$ \overline{\theta}$分别称为置信水平为1-a的双侧置信区间的**置信下限**和**置信上限**，1-a称为**置信水平**。

正态分布$ X \sim N(\mu,\sigma^2)$，$ \sigma^2$已知，$ \mu$未知，设$ X_1,X_2,\cdots,X_n$是来自X的样本，则$ \mu$的置信水平为1-a的置信区间解法为：
可知$ \frac{ \overline{X} - \mu}{ \sigma / \sqrt{n}} \sim N(0,1)$，此式不依赖于任何未知参数，按标准正态分布上a分布点的顶底，有$ P \{ | \frac{ \overline{X} - \mu}{ \sigma / \sqrt{n}}| < z_{\frac{a}{2}}\} = 1-a$，即$ P\{ \overline{X} - \frac{\sigma}{\sqrt{n} } z_{\frac{a}{2}} < \mu < \overline{X} + \frac{\sigma}{\sqrt{n} }z_{\frac{a}{2}}\} = 1-a$，于是，就得到了$ \mu$的一个置信水平为1-a的置信区间$( \overline{X} - \frac{\sigma}{\sqrt{n}} z_{\frac{a}{2}} , \overline{X} + \frac{\sigma}{\sqrt{n}} z_{\frac{a}{2}})$，也可写为$ ( \overline{X} \pm \frac{\sigma}{\sqrt{n}} z_{\frac{a}{2}})$。

置信区间并不是唯一的，但是置信区间短的表示估计的精度高，对于如标准正太分布等，易知对称时区间最短。

寻找未知参数$ \theta$的置信区间的具体操作：
1.寻求一个样本$ X_1,X_2,\cdots,X_n$和$ \theta$的函数$W = W (X_1,X_2,\cdots,X_n ; \theta)$使W的分布不依赖于$ \theta$以及其他未知参数，称具有这种性质得函数W为**枢轴量**。
2.对于给定的置信水平1-a，定出两个常数a，b使得$ P \{ a < W(X_1,X_2,\cdots,X_n ; \theta) < b\} = 1-a$，若能以此得到与之等价的关于$ \theta$的不等式$ \underline{\theta} < \theta < \overline{\theta}$，其中$ \underline{\theta} = \underline{\theta} ( X_1,X_2,\cdots,X_n) , \overline{\theta} = \overline{\theta} ( X_1,X_2,\cdots,X_n)$都是统计量，那么$ ( \underline{\theta} , \overline{\theta} )$就是$ \theta$的一个置信水平为1-a的置信区间。

## 7.5 正态总体均值和方差的区间估计

（一）单个总体$ N( \mu , \sigma^2)$的情况

1.均值$ \mu$的置信区间

(1)$ \sigma^2$已知
$ \frac{ \overline{X} - \mu}{ \sigma / \sqrt{n}} \sim N(0,1)$，置信区间为$ ( \overline{X} \pm \frac{\sigma}{\sqrt{n}} z_{\frac{a}{2}})$。

(2)$ \sigma^2$未知
用$ \sigma^2$的无偏估计量$ S^2$替换。
$ \frac{ \overline{X} - \mu}{ S / \sqrt{n}} \sim t(n-1)$，置信区间为$ ( \overline{X} \pm \frac{S}{\sqrt{n}} t_{\frac{a}{2}} (n-1))$。

2.方差$ \sigma^2$的置信区间($ \mu$未知)
$ \frac{(n-1)S^2}{\sigma^2} \sim \chi^2(n-1)$，置信区间为$ ( \frac{(n-1)S^2}{ \chi_{\frac{a}{2}}^2 (n-1)} , \frac{(n-1)S^2}{ \chi_{ 1 - \frac{a}{2}}^2 (n-1)})$

（二）两个总体$ N(\mu_1,\sigma_1^2),N(\mu_2,\sigma_2^2)$的情况

1.两个总体均值差$ \mu_1 - \mu_2$的置信区间

(1)$ \sigma_1^2 , \sigma_2^2$均为已知，因$ \overline{X} , \overline{Y}$分别为$ \mu_1 , \mu_2$的无偏估计，故$ \overline{X} - \overline{Y}$是$ \mu_1 - \mu_2$的无偏估计。$ \overline{X} - \overline{Y} \sim N(\mu_1 - \mu_2 , \frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2})$，即$ \frac{(\overline{X} - \overline{Y}) - (\mu_1 - \mu_2)}{ \sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}}} \sim N(0,1)$，所以置信区间为$ ( \overline{X} - \overline{Y} \pm z_{\frac{a}{2}} \frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2})$。

(2)$ \sigma_1^2 = \sigma_2 = \sigma^2$但$ \sigma$未知。
$\frac{(\overline{X} - \overline{Y}) - (\mu_1 - \mu_2)}{ S_w \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}} \sim t(n_1+n_2-2)$，置信区间为$ (\overline{X} - \overline{Y} \pm t_{\frac{n}{2}} (n_1+n_2-2) S_w \sqrt{\frac{1}{n_1} + \frac{1}{n_2}})$。其中$ S_w^2 = \frac{ (n_1 - 1) S_1^2 + (n_2 -1 ) S_2^2}{n_1+n_2-2}$

2.两个总体方差比$ \frac{\sigma_1^2}{\sigma_2^2}$的置信区间($ \mu_1 , \mu_2$未知)

$ \frac{S_1^2 / S_2^2}{\sigma_1^2 / \sigma_2^2} \sim F(n_1 - 1 , n_2 - 1)$，置信区间为$ ( \frac{S_1^2}{S_2^2} \frac{1}{F_{\frac{a}{2}} (n_1 -1 ,n_2 -1)} , \frac{S_1^2}{S_2^2} \frac{1}{F_{1 - \frac{a}{2}} (n_1 - 1 , n_2 - 1)})$

注意：$ \chi^2 和 F$分布的曲线不为对称，只能用$ \chi_{ \frac{a}{2}}^2 和 \chi_{ 1 - \frac{a}{2}}^2 ; F_{ \frac{a}{2}} 和 F_{ 1 - \frac{a}{2}}$分别表示。

## 7.6 (0-1)分布参数的区间估计

已知(0-1)分布的均值$ \mu = p$，方差$ \sigma^2 = p(1-p)$，有中心极限定理可得在n充分大时，$ \frac{ \sum_{i = 1}^n X_i - np}{ \sqrt{n p (1-p)}} = \frac{n \overline{X} - np}{ \sqrt{n p (1-p)}}$近似服从$ N(0,1)$分布，于是有$ P\{ -z_{\frac{a}{2}} < \frac{n \overline{X} - np}{ \sqrt{n p (1-p)}} < z_{\frac{a}{2}}\} \approx 1-a$。
解不等式$ -z_{\frac{a}{2}} < \frac{n \overline{X} - np}{ \sqrt{n p (1-p)}} < z_{\frac{a}{2}}$，等价于$ (n+z_{\frac{a}{2}} ^2) p^2 - (2 n \overline{X} + z_{\frac{a}{2}}^2) p + n \overline{X}^2 < 0$。解：
$ p_1 = \frac{1}{2a} (-b - \sqrt{b^2 - 4ac}) , p_2 = \frac{1}{2a} (-b + \sqrt{b^2 - 4ac})$，其中$ a = n+z_{\frac{a}{2}} ^2 , b = - (2 n \overline{X} + z_{\frac{a}{2}}^2) , c = n \overline{X}^2$，所以p的一个被近似的置信水平为1-a的置信区间为$(p_1,p_2)$。

## 7.7 单侧置信区间

对于给定值$a(0 < a < 1)$，若由样本$ X_1,X_2,\cdots,X_n$确定的统计量$ \underline{\theta} = \underline{\theta} (X_1,X_2,\cdots,X_n)$，对于任意$ \theta \in \Theta$满足$ P\{ \theta > \underline{\theta}\} \geqslant 1-a$，则称随机区间$( \underline{\theta} , \infty)$是$ \theta$的置信水平为1-a的**单侧置信区间**，$ \underline{\theta}$称为$ \theta$的置信水平为1-a的**单侧置信下限**。

对于给定值$a(0 < a < 1)$，若由样本$ X_1,X_2,\cdots,X_n$确定的统计量$ \overline{\theta} = \overline{\theta} (X_1,X_2,\cdots,X_n)$，对于任意$ \theta \in \Theta$满足$ P\{ \theta < \overline{\theta}\} \geqslant 1-a$，则称随机区间$(-\infty , \overline{\theta})$是$ \theta$的置信水平为1-a的**单侧置信区间**，$ \overline{\theta}$称为$ \theta$的置信水平为1-a的**单侧置信上限**。

注意：$ \chi^2$分布和$ F$分布无$ -\infty$处，单侧置信上限的起点为0。

# 第8章 假设检验

下述任意检验若带小于等于号或大于等于号，在各检验法中始终是取等号点进行检验，证明：

由下可知我们控制的是第I类错误，即$H_0$为真但拒绝$H_0$的情况，若$H_0$中为等于，那毫无疑问直接取等于；若是小于等于或大于等于的情况，那么我们是检验$ \mu \in \mu_0$的情况，可知$H_0$为真时$ \mu$一定取最大，即等于情况。因此得证：任意检验若带小于等于号或大于等于号，在各检验法中始终是取等号点进行检验。

## 8.1 假设检验

假设检验是在总体分布函数完全未知，或只知其形态、但不知其参数的情况下，为了推断总体的某些未知特性，提出某些关于总体的假设。

例如：给定一系列数值，问方差当前是否满足原标准。那么，提出两个相互对立的假设：$ H_0: \mu = \mu_0 , H_1: \mu \neq \mu_0$。然后，我们给出一个合理的法则，根据这一法则，利用已知样本做出决策是接受假设$ H_0$，还是拒绝假设$ H_0$。由于要检验的假设设计总体均值$ \mu$，由此可以想到用样本均值$ \overline{X}$进行判断。若假设$ H_0$为真，则观察值$ \overline{x}$与$ \mu$的偏差$ | \overline{x} - \mu |$一般不应太大。又因为$ H_0$为真时$ \frac{ \overline{X} - \mu_0}{ \sigma / \sqrt{n}} \sim N(0,1)$，因此可用$ \frac{ | \overline{X} - \mu_0 |}{ \sigma / \sqrt{n}}$衡量$ | \overline{x} - \mu |$的大小。基于此想法，我们可以适当选定一正数k，当观察值$ \overline{x}$满足$ \frac{ | \overline{X} - \mu_0 |}{ \sigma / \sqrt{n}} \geqslant k$时就拒绝假设$ H_0$，反之接受假设$ H_0$。

由于做出决策的依据只是一个样本，因此存在错误的可能性，我们无法排除犯这类错误的可能性，因此希望将犯这类错误的可能性控制在一定限度内，即给出一个较小的数a，使犯这类错误的概率不超过a，即$ P \{ 错误\} \leqslant a$。为了确定上述常数k，对上式右端取等号，即$ P \{ 错误\} = P _{\mu_0} \{ | \frac{ \overline{x} - \mu }{ \sigma / \sqrt{n}} | \geqslant k\} = a$，又因为$ Z = \frac{ \overline{X} - \mu_0}{ \sigma / \sqrt{n}} \sim N(0,1)$所以可知$ k = z_{ \frac{a}{2}}$。于是，当$ |z| = | \frac{ \overline{X} - \mu_0}{ \sigma / \sqrt{n}} | \geqslant k =z _{\frac{a}{2}}$时拒绝假设$ H_0$（有等号），反之接受假设$ H_0$。(a的取值一般为0.01，0.05)。

上述中若$ |z| = | \frac{ \overline{X} - \mu_0}{ \sigma / \sqrt{n}} | \geqslant k$，则称差异是显著的，这时拒绝$ H_0$，反之差异是不显著的，这时接受$ H_0$。数a称为**显著性水平**，统计量$ Z = \frac{ \overline{X} - \mu_0}{ \sigma / \sqrt{n}}$称为**检验统计量**。前面的检测问题通常叙述为：“在显著性水平a下，检验假设$ H_0,H_1$”，也常说为“在显著性水平a下，争对$H_1$检验$ H_0$”。$ H_0$称为**原假设**或**零假设**，$ H_1$称为**备选假设**。当检验统计量取某个区域C中的值时，我们拒绝原假设$ H_0$，则称区域C为**拒绝域**，拒绝域的边界点称为**临界点**。

由于检验法则是根据样本做出的，总有可能做出错误的决策。那么，在假设$ H_0$实际上为真时我们可能犯拒绝$ H_0$的错误，称这类“弃真”的错误为**第I类错误**，又当$ H_0$实际上不真时，我没也有可能接受$ H_0$，称这类“取伪”的错误为**第II类错误**。一般来说，当样本容量固定时，若减小犯一类错误的概率，则犯另一类错误的概率往往增大。，一般来说，我们总是控制犯第I类错误的概率，使它不大于a，这种只对第一类概率加以控制，而不考虑犯第II类错误概率的检验，称为**显著性检验**。

$ H_0 : \mu = \mu_0 , H_1 : \mu \neq \mu_0$称为**双边假设检验**，$ H_0 : \mu \leqslant \mu_0 , H_1 : \mu > \mu_0$称为**双边备泽假设**。若我们需要检验假设：$ H_0 : \mu \leqslant \mu_0 , H_1 : \mu > \mu_0$，称为**右边检验**，$ H_0 : \mu \geqslant \mu_0 , H_1 : \mu < \mu_0$，称为**左边检验**。左边检验和右边检验统称为**单边检验**。

单边检验的拒绝域：
1.右边检验：$ z = \frac{ \overline{x} - \mu_0}{ \sigma / \sqrt{n}} \geqslant z_a$。
2.左边检验：$ z = \frac{ \overline{x} - \mu_0}{ \sigma / \sqrt{n}} \leqslant - z_a$。

处理参数的假设检验问题步骤如下:
1.根据实际问题的要求，提出原假设$ H_0$及备择假设$ H_1$。
2.给定显著性水平a预计样本容量n。
3.给定检验统计量以及拒绝域的形式。
4.按P{当$ H_0$为真拒绝$ H_0$}$ \leqslant$a求出拒绝域。
5.取样，根据样本观察值作出决策，是接受$ H_0$还是拒绝$ H_0$。

## 8.2 正态总体均值的假设检验

（一）单个总体均值$ \mu$的检验

原假设$ H_0 : \mu = \mu_0$，备择假设$ H_1 : \mu \neq \mu_0$。

1.$ \sigma^2$已知，关于$\mu$的检验（Z检验)

利用统计量$ Z = \frac{ \overline{X} - \mu_0}{ \sigma / \sqrt{n}}$来确定拒绝域：$ |z| = | \frac{ \overline{X} - \mu_0}{ \sigma / \sqrt{n}} | \geqslant k =z _{\frac{a}{2}}$。

2.$ \sigma^2$未知，关于$ \mu$的检验（t检验）

利用统计量$ t = \frac{ \overline{X} - \mu_0}{ S / \sqrt{n}}$来确定拒绝域：$ |t| = | \frac{ \overline{X} - \mu_0}{ S / \sqrt{n}} | \geqslant k =t _{\frac{a}{2}} (n-1)$。

（二）两个正态总体均值差的检验（t检验）

原假设$ H_0 : \mu_1 - \mu_2 = \delta$，备择假设$ H_1 : \mu_1 - \mu_2 \neq \delta$。（或原假设$ H_0 : \mu_1 - \mu_2 \leqslant \delta$，备择假设$ H_1 : \mu_1 - \mu_2 > \delta$）

1.我们还可以用t检测法检验具有**相同方差**的两正态总体均值差的假设。

利用统计量$ t = \frac{ ( \overline{X} - \overline{Y} ) - \delta }{ S_w \sqrt{ \frac{1}{n_1} + \frac{1}{n_2}}}$来确定拒绝域，$ | t | = \frac{ | ( \overline{X} - \overline{Y} ) - \delta | }{ S_w \sqrt{ \frac{1}{n_1} + \frac{1}{n_2}}} \geqslant t_{ \frac{a}{2}} (n_1+n_2-2)$.

2.当两个正态总体的方差均为已知，我们可用Z检验法来检验两正态总体均值差的假设问题。

利用统计量$ Z = \frac{ (\overline{X} - \overline{Y}) - (\mu_1 - \mu_2))}{ \sqrt{ \frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}}}$来确定拒绝域。（同（一）.1）

（三）基于成对数据的检验（t检验）

有时为了比较两种产品等两种物品的差异，我们常在相同的条件下做对比试验，得到一批成对的观察值，然后分析观察数据做出判断，这种方法常称为**逐队比较法**。

一般，设有n队相互独立的观察结果：$ (X_1,Y_1),(X-2,Y_2),\cdots,(X_n,Y_n)$，令$ D_1 = X_1 - Y_1 , D_2 = X_2 - Y_2 , \cdots , D_n = X_n - Y_n$，则$ D_1 , D_2 , \cdots , D_n$是由同一因素引起的，可认为他们服从同一分布。

$ t = \frac{ \overline{d}}{s_D / \sqrt{n}}, \overline{d}为样本均值，s_D^2为样本方差$。

## 8.3 正态总体方差的假设检验

（一）单个总体的情况

|   |                                                               原假设$ H_0$                                                                |                                                                             检验统计量                                                                              |                                      备择假设$ H_1$                                       |                                                                                                   拒绝域                                                                                                   |
|:-:|:-----------------------------------------------------------------------------------------------------------------------------------------:|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:-----------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|
| 1 |                       $ \mu \leqslant \mu_0$ <br> $ \mu \geqslant \mu_0$ <br> $ \mu = \mu_0$ <br> ($\sigma^2$已知)                        |                                                       $ Z = \frac{ \overline{X} - \mu_0}{ \sigma / \sqrt{n}}$                                                       |                 $ \mu > \mu_0$ <br> $ \mu < \mu_0$ <br> $ \mu \neq \mu_0$                 |                                                        $ z \geqslant z_a$ <br> $ z \leqslant - z_a$ <br> $ \mid z \mid \geqslant z_{ \frac{a}{2}}$                                                         |
| 2 |                       $ \mu \leqslant \mu_0$ <br> $ \mu \geqslant \mu_0$ <br> $ \mu = \mu_0$ <br> ($\sigma^2$未知)                        |                                                         $ t = \frac{ \overline{X} - \mu_0}{ S / \sqrt{n}}$                                                          |                 $ \mu > \mu_0$ <br> $ \mu < \mu_0$ <br> $ \mu \neq \mu_0$                 |                                                 $ t \geqslant t_a(n-1)$ <br> $ t \leqslant - t_a(n-1)$ <br> $ \mid t \mid \geqslant t_{ \frac{a}{2}}(n-1)$                                                 |
| 3 | $ \mu_1 - \mu_2 \leqslant \delta$ <br> $ \mu_1 - \mu_2 \geqslant \delta$ <br> $\mu_1 - \mu_2 = \delta$ <br> ($\sigma^2_1 , \sigma_2$已知) |                            $ Z = \frac{ \overline{X} - \overline{Y} - \delta}{ \sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}}}$                             | $ \mu_1 - \mu_2 > \delta$ <br> $ \mu_1 - \mu_2 < \delta$ <br> $\mu_1 - \mu_2 \neq \delta$ |                                                        $ z \geqslant z_a$ <br> $ z \leqslant - z_a$ <br> $ \mid z \mid \geqslant z_{ \frac{a}{2}}$                                                         |
| 4 | $ \mu_1 - \mu_2 \leqslant \delta$ <br> $ \mu_1 - \mu_2 \geqslant \delta$ <br> $\mu_1 - \mu_2 = \delta$ <br> ($\sigma^2_1 , \sigma_2$已知) | $ t = \frac{ \overline{X} - \overline{Y} - \delta}{ S_w \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}}$ <br> $ S_w^2 = \frac{(n_1 - 1)S_1^2 + (n_2 - 1)S_2^2}{n_1 + n_2 -2}$ | $ \mu_1 - \mu_2 > \delta$ <br> $ \mu_1 - \mu_2 < \delta$ <br> $\mu_1 - \mu_2 \neq \delta$ |                                $ t \geqslant t_a(n_1  + n_2 - 2)$ <br> $ t \leqslant - t_a(n_1  + n_2 - 2)$ <br> $ \mid t \mid \geqslant t_{ \frac{a}{2}}(n_1  + n_2 - 2)$                                 |
| 5 |           $ \sigma^2 \leqslant \sigma_0^2$ <br> $ \sigma^2 \geqslant \sigma_0^2$ <br> $ \sigma^2 = \sigma_0^2$ <br> ($\mu$未知)           |                                                               $ \chi^2 = \frac{(n-1)S^2}{\sigma_0^2}$                                                               |  $ \sigma^2 > \sigma_0^2$ <br> $ \sigma^2 < \sigma_0^2$ <br> $ \sigma^2 \neq \sigma_0^2$  |         $ \chi^2 \geqslant \chi_a^2 (n-1)$ <br> $ \chi^2 \leqslant \chi_{ 1 - a} (n-1)$ <br> $ \chi^2 \geqslant \chi_{\frac{a}{2}} ^2 (n-1)$或$ \chi^2 \leqslant \chi ^2_{ 1 - \frac{a}{2}} (n-1)$         |
| 6 |      $ \sigma^2 \leqslant \sigma_0^2$ <br> $ \sigma^2 \geqslant \sigma_0^2$ <br> $ \sigma^2 = \sigma_0^2$ <br> ($\mu_1 , \mu_2$已知)      |                                                                     $ F = \frac{S_1^2}{S_2^2}$                                                                      |  $ \sigma^2 > \sigma_0^2$ <br> $ \sigma^2 < \sigma_0^2$ <br> $ \sigma^2 \neq \sigma_0^2$  | $ F \geqslant F_a (n_1 - 1 , n_2 - 1)$ <br> $ F \leqslant F_{a} (n_1 - 1 , n_2 - 1)$ <br> $ F \geqslant F_{\frac{a}{2}} ^2 (n_1 - 1 , n_2 - 1)$或$ F \geqslant F_{1 - \frac{a}{2}} ^2 (n_1 - 1 , n_2 - 1)$ |
| 7 |                             $ \mu_D \leqslant 0$ <br> $ \mu_D \geqslant 0$ <br> $ \mu_D = 0$ <br> (成对数据)                              |                                                          $ t = \frac{ \overline{D} - 0}{ S_D / \sqrt{n}}$                                                           |                    $ \mu_D > 0$ <br> $ \mu_D < 0$ <br> $ \mu_D \neq 0$                    |                                                 $ t \geqslant t_a(n-1)$ <br> $ t \leqslant - t_a(n-1)$ <br> $ \mid t \mid \geqslant t_{ \frac{a}{2}}(n-1)$                                                 |

（二）两个总体的情况

设$ X_1,X_2,\cdots,X_{n_1}$是来自总体$ N( \mu_1 , \sigma_1^2)$的样本，$ Y_1,Y_2,\cdots,Y_{n_2}$是来自总体$ N( \mu_2 , \sigma_2^2)$的样本，且两样本独立，其样本方差分别为$ S_1^2 , S_2^2$，且设$ \mu_1,\mu_2,\sigma_1^2,\sigma_2^2$均为未知，现在需要检测假设$ H_0: \sigma_1^2 \leqslant \sigma_2^2 , H_1: \sigma_1^2 > \sigma_2^2$

利用统计量$ F = \frac{S_1^2}{S_2^2}$求解。（检验的临界点为$ \sigma_1^2 = \sigma_2^2$，所以F分布的分母为1）。
