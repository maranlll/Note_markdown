# 第5章 大数定理及中心极限定理

## 5.1 大数定理

**弱大数定理(辛钦大数定理)：**
设$X_1,X_2,\cdots,X_n$是相互独立，服从同一分布的随机变量序列，且具有数学期望$E(X_k) = \mu (k = 1,2,\cdots )$，作前n个变量的算术平均$\frac{1}{n} \sum_{ k = 1}^{n}X_k$，对任意$\varepsilon > 0$，有$ \lim \limits _{n \to \infty} P \{ | \frac{1}{n} \sum _{k = 1 } ^{n} X_k - \mu | < \varepsilon \} = 1$

证：
$E(\frac{1}{n} \sum _{ k = 1 } ^{n} X_k ) = \frac{1}{n} \sum _{ k = 1 } ^{n} E(X_k) = \mu$。
$D(\frac{1}{n} \sum _{ k = 1 } ^{n} X_k) = \frac{1}{n^2} \sum_{ k = 1 } ^{n} D(X_k) = \frac{\sigma ^2}{n}$。
由切比雪夫不等式得：
$ 1 \geq P\{ | \frac{1}{n} \sum _{ k = 1 }^{n} X_k -\mu | < \varepsilon \} \geq 1 - \frac{ \frac{\sigma ^2}{n}}{\varepsilon^2}$。由$ n \to \infty$可知，$ \lim \limits _{n \to \infty} P \{ | \frac{1}{n} \sum _{k = 1 } ^{n} X_k - \mu | < \varepsilon \} = 1$。

此式表明，对任意正数$\varepsilon$，当n充分大时，不等式$ |\frac{1}{n} \sum _{k = 1} ^{n} X_k - \mu | < \varepsilon$成立的概率很大。通俗的说，辛钦大数定理是说，对于独立同分布且具有均值$\mu$的随机变量$ X_1,X_2,\cdots,X_n$，当n很大时他们的算术平均$\frac{1}{n} \sum _{k = 1} ^{n} X_k$很可能接近于$\mu$。

设$ Y_1,Y_2,\cdots,Y_n$是一个随机变量序列，a是一个常数，若对于任意正数$\varepsilon$，有$ \lim \limits _{n \to \infty} P \{ | Y_n - a | < \varepsilon \} = 1$，则称序列$ Y_1,Y_2,\cdots,Y_n$**依概率收敛于a**，记为$ Y_n \underrightarrow{P} a$.

依概率收敛的序列有以下性质：
设$X_n \underrightarrow{P} a , Y_n \underrightarrow{P} b$，又设函数g(x,y)在点(a,b)连续，则$g(X_n,Y_n) \underrightarrow{P} g(a,b)$.

因此，辛钦大数定理又可叙述为：设变量$X_1,X_2,\cdots,X_n$相互独立，服从同一分布且具有数学期望$E(X_k) = \mu ,k = 1,2,\cdots$，则序列$ \overline{X} = \frac{1}{n} \sum _{k = 1} ^{n} X_k$依概率收敛于$\mu$，即$ \overline{X} \underrightarrow{P} \mu$.

推论：
**伯努利大数定理：**
设$f_A$是n次独立重复实验中事件A发生的次数，p是事件A在每次实验中发生的概率，则对任意正数$\varepsilon$，有$\lim \limits _{ n \to \infty} P \{ | \frac{f_A}{n} - p | < \varepsilon \} = 1 或 \lim \limits _{ n \to \infty} P \{ | \frac{f_A}{n} - p | \geq \varepsilon \} = 0$

表示在n充分大时，事件“频率$\frac{f_A}{n}$与概率p的偏差小于$\varepsilon$”实际上几乎是必定要发生的。这就是我们所说的概率稳定性的真正含义。在实际应用中，当试验次数很大时，便可以用事件的频率代替事件的概率。

## 5.2 中心极限定理

**定理一（独立同分布的中心极限定理）：**
设随机变量$X_1,X_2,\cdots,X_n$相互独立，服从同一分布，且具有数学期望和方差：$E(X_k) = \mu,D(X_k) = \sigma^2 > 0 , k = 1,2,\cdots$，则随机变量和$ \sum _{k = 1} ^n X_k$的标准化变量$ Y_n = \frac{\sum_{ k = 1}^n X_k - E(\sum_{k = 1}^n X_k)}{ \sqrt{D( \sum _{k = 1}^n X_k)}} = \frac{\sum_{k = 1}^n X_k - n \mu}{ \sqrt{n} \sigma}$在n充分大时，有$ \frac{ \sum _{k = 1} ^ n X_k - n \mu }{ \sigma \sqrt{n}} \sim N(0,1)$，可转化为$ \frac{ \frac{1}{n} \sum _{k = 1} ^ n X_k - \mu }{ \frac{\sigma } {\sqrt{n}} }\sim N(0,1)$，即$ \frac{ \overline{X} - \mu }{ \frac{\sigma } {\sqrt{n}} } \sim N(0,1)$ 或 $ \overline{X} \sim N(\mu , \frac{\sigma}{\sqrt{n}})$。

也就是说，均值为$\mu$，方差为$\sigma^2$地独立同分布的随机变量$X_1,X_2,\cdots,X_n$的算术平均值$ \overline{X}$在n充分大时近似地服从均值为$\mu$，方差为$\frac{\sigma^2}{n}$的正态分布，这一结果是数理统计中大样本统计推断的基础。

此式中:$Y_n$的分布函数$F_n(x)$对任意x满足$ \lim \limits _{n \to \infty}  F_{n}(x) = \lim \limits _{n \to \infty} P \{ \frac{ \sum _{k = 1}^n X_k - n \mu}{\sqrt{n) \sigma} } \leqslant x \} = \Phi(x)$

**定理二（李雅普诺夫定理）：**
设随机变量$X_1,X_2,\cdots,X_n$相互独立，他们具有数学期望和方差$E(X_k) = \mu _k , D(X_k) = \sigma _k^2 > 0 , k = 1,2,\cdots$，记$ B_n ^ 2 = \sum _{ k = 1} ^n \sigma_k ^2$，则随机变量之和$ \sum _{k = 1} ^ n X_k$的标准化变量$ Z_n = \frac{\sum _{k = 1}^n - E(\sum _{k = 1}^n X_k)}{ \sqrt{D(\sum _{k = 1}^n X_k)}} = \frac{ \sum _{k = 1}^n X_k - \sum _{k = 1}^n \mu _k}{B_n}$,可知在n很大时，近似地服从正态分布N(0,1)，由此，当n很大时，$ \sum _{k = 1}^n X_k = B_n Z_n + \sum_{k = 1}^n \mu_k$近似的服从正态分布$ N(\sum _{k = 1}^n \mu_k , B_n ^2)$。也就是说，无论各个随机变量$X_k , k = 1,2,\cdots$服从什么分布，只要满足定理的条件，那么他们的和$\sum _{k = 1}^n$当n很大时，就近似地服从正态分布。

此式中:$Y_n$的分布函数$F_n(x)$对任意x满足$ \lim \limits _{n \to \infty}  F_{n}(x) = \lim \limits _{n \to \infty} P \{ \frac{ \sum _{k = 1}^n X_k - \sum_{k = 1} ^n \mu_k}{B_n ^2} \leqslant x \} = \Phi(x)$。

**定理三（棣莫弗-拉普拉斯定理）：**
此定理为定理一的特殊情况：
设随机变量$ \eta _{n} ,n = 1,2,\cdots$服从参数n，p( 0 < p < 1)的二项分布。由于二项分布$ \eta _{n}$可以拆分成n个相互独立，服从同一(0-1)分布的诸随机变量$ X_1,X_2,\cdots,X_n$之和，即有$ \eta_n = \sum _{k = 1}^n X_k,E(X_k) = p,D(X_k) = p(1-p)$,所以有定理一可得：$ \eta_{n}$的分布函数$F_n(x)$对任意x满足$ \lim \limits _{n \to \infty} F_{n}(x) = \lim \limits _{n \to \infty} P \{ \frac{ \eta _{n} - np}{\sqrt{np(1-p)} } \leqslant x \} = \Phi(x)$。

这个定理表明，正态分布是二项分布的极限分布，当n充分大时，我们可以利用此式来计算二项分布的概率。